/home/anwar/Desktop/chatGPT/.venv/bin/python /home/anwar/Desktop/chatGPT/main.py 

================================================================================
SLIDING-WINDOW SPARSE ATTENTION IN GPT MODELS
Practical Implementation and Empirical Evaluation
================================================================================

Author: Anwar Sleiman Haidar
Course: EN.705.743 - ChatGPT from Scratch
Institution: Johns Hopkins University
Date: November 18, 2025

================================================================================
DEPENDENCY CHECK
================================================================================
✓ matplotlib available
✓ seaborn available
✓ scipy available
✓ tqdm available
✓ CUDA available: NVIDIA RTX PRO 6000 Blackwell Workstation Edition
  GPU Memory: 101.94 GB

================================================================================
DATA FILE CHECK
================================================================================
✓ training_data.npy
✓ gpt.py
✓ gpt_win.py
✓ embedding.py
✓ linear.py
✓ train_model.py
✓ compare_models_10runs.py
✓ create_plots_improved.py
✓ metrics_utils.py

================================================================================
EXPERIMENT CONFIGURATION
================================================================================

This experiment will:

1. Run 10 randomized comparisons between baseline and sparse attention GPT
2. Train each model on Shakespeare text data
3. Track training time, memory usage, and model quality
4. Perform statistical analysis (t-tests, Cohen's d)
5. Generate 8 publication-quality plots
6. Save all metrics for later analysis

Model Configuration:
  - Architecture: GPT with 8 layers, 16 heads, d_model=512
  - Training: 8 batch size, 3e-4 learning rate, 500 warmup steps
  - Sparse attention: window_size=96, dilation=4
  
Expected Results:
  - Training speedup: 5-10%
  - Memory reduction: 4-5%
  - Quality difference: <1% (negligible)
  
Expected Runtime:
  - Per single run (both models): ~30 minutes
  - Total for 10 runs: ~5 hours
  - GPU Required: NVIDIA GPU with 8GB+ VRAM
  - CPU: Not supported (would take 50+ hours)

Statistical Validation:
  - 10 runs provide high confidence (n=10)
  - Randomized order prevents bias
  - t-tests for significance (α=0.05)
  - Cohen's d for effect size
    
================================================================================

WARNING: This experiment takes approximately 5 HOURS to complete!
Each run takes ~30 minutes, and we perform 10 runs for statistical validity.

Ready to start experiment? This will take ~5 hours. [y/N]: y

================================================================================
STARTING EXPERIMENT
================================================================================

================================================================================
GPT MODEL COMPARISON - 10 RUNS
Multiple runs with GPU cleanup and order randomization
================================================================================

Loading training data...
Loaded data shape: (510079, 257)
Dataset size: 510079 sequences, seq_len=256
Inferred vocab size: 10000
Using device: cuda

Model Configuration:
  d_model: 512
  n_heads: 16
  layers: 8
  batch_size: 8
  learning_rate: 0.0003
  warmup_steps: 500

Window Model Parameters:
  window_size: 96 (reduced for more sparsity)
  dilation: 4

Experimental Setup:
  Number of runs: 10
  Randomize order: True

================================================================================
RUN 1 / 10
================================================================================
Order: Baseline → Window

================================================================================
TRAINING BASELINE MODEL (Run 1)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:34<00:00, 68.19it/s, step=63760, loss=3.5324]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 934.99s (15.58 min)
  Peak GPU memory: 1461.43 MB
  Final loss: 3.5324
  Avg loss (last 100): 3.3128

Cleaning up after Baseline training...

================================================================================
TRAINING WINDOW MODEL (Run 1)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:38<00:00, 72.56it/s, step=63760, loss=3.4699]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 878.70s (14.64 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.4699
  Avg loss (last 100): 3.3488

Saved window_model_weights.pt

Cleaning up after Window training...

================================================================================
RUN 1 COMPLETED in 1824.28s (30.40 min)
================================================================================

================================================================================
RUN 2 / 10
================================================================================
Order: Window → Baseline

================================================================================
TRAINING WINDOW MODEL (Run 2)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:39<00:00, 72.52it/s, step=63760, loss=3.4737]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 879.19s (14.65 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.4737
  Avg loss (last 100): 3.3405

Cleaning up after Window training...

================================================================================
TRAINING BASELINE MODEL (Run 2)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:40<00:00, 67.81it/s, step=63760, loss=3.6545]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 940.22s (15.67 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.6545
  Avg loss (last 100): 3.3307

Cleaning up after Baseline training...

================================================================================
RUN 2 COMPLETED in 1829.10s (30.49 min)
================================================================================

================================================================================
RUN 3 / 10
================================================================================
Order: Baseline → Window

================================================================================
TRAINING BASELINE MODEL (Run 3)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:38<00:00, 67.91it/s, step=63760, loss=3.2208]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 938.89s (15.65 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.2208
  Avg loss (last 100): 3.3455

Cleaning up after Baseline training...

================================================================================
TRAINING WINDOW MODEL (Run 3)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:39<00:00, 72.52it/s, step=63760, loss=3.5315]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 879.17s (14.65 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.5315
  Avg loss (last 100): 3.3395

Cleaning up after Window training...

================================================================================
RUN 3 COMPLETED in 1827.79s (30.46 min)
================================================================================

================================================================================
RUN 4 / 10
================================================================================
Order: Window → Baseline

================================================================================
TRAINING WINDOW MODEL (Run 4)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:38<00:00, 72.54it/s, step=63760, loss=3.4910]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 878.97s (14.65 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.4910
  Avg loss (last 100): 3.3692

Cleaning up after Window training...

================================================================================
TRAINING BASELINE MODEL (Run 4)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:37<00:00, 68.03it/s, step=63760, loss=3.5067]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 937.24s (15.62 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.5067
  Avg loss (last 100): 3.3338

Cleaning up after Baseline training...

================================================================================
RUN 4 COMPLETED in 1826.00s (30.43 min)
================================================================================

================================================================================
RUN 5 / 10
================================================================================
Order: Baseline → Window

================================================================================
TRAINING BASELINE MODEL (Run 5)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:37<00:00, 68.00it/s, step=63760, loss=3.3262]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 937.62s (15.63 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.3262
  Avg loss (last 100): 3.3232

Cleaning up after Baseline training...

================================================================================
TRAINING WINDOW MODEL (Run 5)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:39<00:00, 72.52it/s, step=63760, loss=3.2374]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 879.17s (14.65 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.2374
  Avg loss (last 100): 3.3446

Cleaning up after Window training...

================================================================================
RUN 5 COMPLETED in 1826.57s (30.44 min)
================================================================================

================================================================================
RUN 6 / 10
================================================================================
Order: Baseline → Window

================================================================================
TRAINING BASELINE MODEL (Run 6)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:37<00:00, 68.03it/s, step=63760, loss=3.5399]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 937.17s (15.62 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.5399
  Avg loss (last 100): 3.3295

Cleaning up after Baseline training...

================================================================================
TRAINING WINDOW MODEL (Run 6)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:38<00:00, 72.54it/s, step=63760, loss=3.2080]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 878.97s (14.65 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.2080
  Avg loss (last 100): 3.3597

Cleaning up after Window training...

================================================================================
RUN 6 COMPLETED in 1826.04s (30.43 min)
================================================================================

================================================================================
RUN 7 / 10
================================================================================
Order: Baseline → Window

================================================================================
TRAINING BASELINE MODEL (Run 7)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:38<00:00, 67.97it/s, step=63760, loss=3.3824]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 938.09s (15.63 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.3824
  Avg loss (last 100): 3.3466

Cleaning up after Baseline training...

================================================================================
TRAINING WINDOW MODEL (Run 7)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:38<00:00, 72.55it/s, step=63760, loss=3.5551]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 878.84s (14.65 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.5551
  Avg loss (last 100): 3.3569

Cleaning up after Window training...

================================================================================
RUN 7 COMPLETED in 1826.78s (30.45 min)
================================================================================

================================================================================
RUN 8 / 10
================================================================================
Order: Window → Baseline

================================================================================
TRAINING WINDOW MODEL (Run 8)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:38<00:00, 72.56it/s, step=63760, loss=3.7085]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 878.72s (14.65 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.7085
  Avg loss (last 100): 3.3462

Cleaning up after Window training...

================================================================================
TRAINING BASELINE MODEL (Run 8)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:36<00:00, 68.10it/s, step=63760, loss=3.1833]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 936.27s (15.60 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.1833
  Avg loss (last 100): 3.3127

Cleaning up after Baseline training...

================================================================================
RUN 8 COMPLETED in 1824.90s (30.41 min)
================================================================================

================================================================================
RUN 9 / 10
================================================================================
Order: Baseline → Window

================================================================================
TRAINING BASELINE MODEL (Run 9)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:36<00:00, 68.10it/s, step=63760, loss=3.0631]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 936.26s (15.60 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.0631
  Avg loss (last 100): 3.3355

Cleaning up after Baseline training...

================================================================================
TRAINING WINDOW MODEL (Run 9)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:38<00:00, 72.55it/s, step=63760, loss=3.0137]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 878.87s (14.65 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.0137
  Avg loss (last 100): 3.3415

Cleaning up after Window training...

================================================================================
RUN 9 COMPLETED in 1825.01s (30.42 min)
================================================================================

================================================================================
RUN 10 / 10
================================================================================
Order: Baseline → Window

================================================================================
TRAINING BASELINE MODEL (Run 10)
================================================================================
Performing aggressive GPU cleanup...
Baseline model has 35,583,760 parameters.

Training Baseline...
Warming up GPU for Baseline...
Starting timed training for Baseline...
Training Baseline: 100%|██████████| 63760/63760 [15:36<00:00, 68.10it/s, step=63760, loss=3.5050]

Baseline Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 936.32s (15.61 min)
  Peak GPU memory: 1465.49 MB
  Final loss: 3.5050
  Avg loss (last 100): 3.3388

Cleaning up after Baseline training...

================================================================================
TRAINING WINDOW MODEL (Run 10)
================================================================================
Performing aggressive GPU cleanup...
Window model has 35,583,760 parameters.

Training Window...
Warming up GPU for Window...
Starting timed training for Window...
Training Window: 100%|██████████| 63760/63760 [14:38<00:00, 72.56it/s, step=63760, loss=3.3829]

Window Training Complete!
  Total steps: 63760
  Total tokens: 130,580,224
  Training time: 878.68s (14.64 min)
  Peak GPU memory: 1398.49 MB
  Final loss: 3.3829
  Avg loss (last 100): 3.3412

Cleaning up after Window training...

================================================================================
RUN 10 COMPLETED in 1824.93s (30.42 min)
================================================================================

================================================================================
TIMING SUMMARY
================================================================================

Total experiment time: 18261.63s (304.36 min)
Average time per run: 1826.14s (30.44 min)
Run time range: 1824.28s - 1829.10s

================================================================================
AGGREGATED RESULTS
================================================================================

Metric                         Baseline                  Window                    Improvement         
----------------------------------------------------------------------------------------------------
Training Time (s)               937.31 ± 1.42             878.93 ± 0.19                          +6.23%
Peak GPU Memory (MB)           1465.09 ± 1.22            1398.49 ± 0.00                          +4.55%
Avg Loss (last 100)             3.3309 ± 0.0113           3.3488 ± 0.0094                        +0.54%

================================================================================
STATISTICAL SIGNIFICANCE
================================================================================

Training Time Difference:
  t-statistic: 121.9982
  p-value: < 0.0001 (extremely significant)
  ✓ Statistically significant at p < 0.05
  Cohen's d (effect size): 57.5105

================================================================================
CONCLUSION
================================================================================

With 10 run(s) and randomized order:
  • Training speed improvement: +6.23%
  • Memory usage improvement: +4.55%
  • Quality difference: +0.54%

 SUCCESS: Window model is faster with comparable quality!

Saved improved_comparison_results.txt

✓ Saved metrics to output/experiment_results.npz
  - 10 baseline runs
  - 10 window runs
  - Human-readable version: output/experiment_results.json

================================================================================
CREATING 8 PUBLICATION-QUALITY PLOTS (10 runs)
================================================================================

1. Training loss curves with mean...
  Saved plots/1_training_curves.png
2. Final 100 losses comparison...
  Saved plots/2_final_losses.png
3. Metrics bar charts...
  Saved plots/3_metrics_comparison.png
4. Speed-quality tradeoff...
  Saved plots/4_speedup_quality.png
5. Statistical distributions...
  Saved plots/5_distributions.png
6. Per-run breakdown...
  Saved plots/6_per_run.png
7. Improvement summary...
  Saved plots/7_summary.png
8. Loss convergence with confidence bands...
  Saved plots/8_convergence.png

================================================================================
* All 8 plots saved to 'plots/'
================================================================================


================================================================================
EXPERIMENT COMPLETED SUCCESSFULLY!
================================================================================

Generated outputs:
  ✓ output/experiment_results.npz - Saved metrics
  ✓ output/experiment_results.json - Human-readable metrics
  ✓ plots/ - 8 visualization plots
  ✓ output/improved_comparison_results.txt - Text summary

Next steps:
  1. Check plots/ directory for visualizations
  2. Review markdown/improved_comparison_results.txt for summary
  3. Use recreate_plots.py to regenerate plots with different styling

To recreate plots anytime:
  python recreate_plots.py


Process finished with exit code 0