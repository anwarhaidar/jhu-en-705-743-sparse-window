INSTALLATION INSTRUCTIONS
=========================

This project requires Python 3.8+ and several common scientific computing libraries.

REQUIRED DEPENDENCIES
--------------------

1. PyTorch (with CUDA support recommended)
2. NumPy
3. Matplotlib
4. Seaborn
5. SciPy
6. tqdm

INSTALLATION OPTIONS
-------------------

Option 1: Using pip (Recommended)
----------------------------------

pip install torch numpy matplotlib seaborn scipy tqdm

If on Linux/Unix system that requires --break-system-packages:

pip install torch numpy matplotlib seaborn scipy tqdm --break-system-packages


Option 2: Using conda
---------------------

conda install pytorch numpy matplotlib seaborn scipy tqdm -c pytorch


Option 3: Install from requirements file
-----------------------------------------

Create a file named requirements.txt with:

torch>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
scipy>=1.10.0
tqdm>=4.65.0

Then run:

pip install -r requirements.txt


CUDA/GPU SETUP
--------------

For GPU acceleration (highly recommended):

1. Check CUDA availability:
   python -c "import torch; print(torch.cuda.is_available())"

2. If False, install PyTorch with CUDA:
   
   For CUDA 11.8:
   pip install torch --index-url https://download.pytorch.org/whl/cu118
   
   For CUDA 12.1:
   pip install torch --index-url https://download.pytorch.org/whl/cu121
   
   See https://pytorch.org/get-started/locally/ for other versions


VERIFICATION
-----------

After installation, verify all dependencies:

python -c "import torch; import numpy; import matplotlib; import seaborn; import scipy; from tqdm import tqdm; print('All dependencies installed successfully!')"


DATASET
-------

The training_data.npy file (Shakespeare text dataset) should be included
in the project directory. If missing:

1. The dataset is tokenized Shakespeare text
2. You can recreate it from the provided shakespeare.txt if available
3. Or use any other text dataset converted to numpy format


HARDWARE REQUIREMENTS
--------------------

Minimum:
- CPU: Any modern x86-64 processor
- RAM: 8 GB
- Storage: 1 GB

Recommended:
- CPU: Multi-core processor (4+ cores)
- GPU: NVIDIA GPU with 8+ GB VRAM (RTX 3060 or better)
- RAM: 16 GB
- Storage: 2 GB


TROUBLESHOOTING
--------------

1. Import errors:
   - Ensure all packages are installed
   - Check Python version (3.8+)
   - Try upgrading pip: pip install --upgrade pip

2. CUDA out of memory:
   - Reduce batch_size in main.py (e.g., from 8 to 4)
   - Close other GPU-using applications

3. Slow performance:
   - Ensure CUDA is available and being used
   - Check GPU utilization: nvidia-smi
   - Consider using smaller model if on CPU


OPTIONAL: Virtual Environment
-----------------------------

It's recommended to use a virtual environment:

# Create virtual environment
python -m venv venv

# Activate (Linux/Mac)
source venv/bin/activate

# Activate (Windows)
venv\Scripts\activate

# Install dependencies
pip install torch numpy matplotlib seaborn scipy tqdm

# Verify
python main.py


TESTING INSTALLATION
-------------------

Run a quick test:

python -c "from compare_models_10runs import compare_models_improved; print('Installation successful!')"


RUNNING THE EXPERIMENT
---------------------

Once installed, simply run:

python main.py

This will:
1. Check all dependencies
2. Verify required files
3. Run the full 10-iteration experiment
4. Generate plots and statistics


ESTIMATED RUNTIME
----------------

GPU (RTX 3090/4090): 20-30 minutes
GPU (RTX 3060): 30-40 minutes  
GPU (GTX 1080): 40-60 minutes
CPU: 3-5 hours (not recommended)


SUPPORT
-------

If you encounter issues:
1. Check the README.md for detailed documentation
2. Ensure all files are in the same directory
3. Verify CUDA installation if using GPU
4. Check PyTorch installation: python -c "import torch; print(torch.__version__)"


SYSTEM TESTED ON
---------------

This code has been tested on:
- Ubuntu 24.04 LTS with CUDA 12.9
- PyTorch 2.8
- NVIDIA RTX PRO 6000 Blackwell


ADDITIONAL NOTES
---------------

- No custom CUDA kernels required - pure PyTorch implementation
- Works on CPU but significantly slower
- All operations use standard PyTorch APIs
- Fully portable and reproducible across systems
